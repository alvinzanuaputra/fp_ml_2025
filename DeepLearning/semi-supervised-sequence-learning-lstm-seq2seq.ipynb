{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Sequence Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this project, I employed two innovative approaches to enhance sequence learning in Recurrent Neural Networks (RNNs).\n",
    "\n",
    "Initially, I trained a model for sentiment analysis on the IMDB Review dataset using a common LSTM network with two LSTM layers. Following this, I implemented a highly effective machine learning technique known as the Sequence-to-Sequence (seq2seq) model. In this technique, I trained a seq2seq model and used its LSTM layer weights to initialize the main sequence model for sentiment analysis. This approach leverages the abundance of unlabeled datasets, allowing us to train an unsupervised network more easily and subsequently use the resulting weights to improve the performance of our primary project.\n",
    "\n",
    "The second approach involved using a language model. I experimented with two different language models for this task. The first approach utilized a pretrained language model, while the second approach involved building a language model from scratch and then using its weights for the main sequence learning project.\n",
    "\n",
    "By incorporating these techniques, the project aims to enhance the stability and effectiveness of LSTM networks for sentiment analysis.\n",
    "\n",
    "Paper Link : <a href=\"https://paperswithcode.com/paper/semi-supervised-sequence-learning\">Semi-Supervised Sequence Learning</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b726de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
      "License(s): other\n",
      "Downloading imdb-dataset-of-50k-movie-reviews.zip to c:\\Users\\ASUS\\Desktop\\KULIAH\\Pembelajaran Mesin\\FP\\DeepLearning\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/25.7M [00:00<?, ?B/s]\n",
      "100%|██████████| 25.7M/25.7M [00:00<00:00, 749MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading rotten-tomatoes-movies-and-critic-reviews-dataset.zip to c:\\Users\\ASUS\\Desktop\\KULIAH\\Pembelajaran Mesin\\FP\\DeepLearning\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/77.2M [00:00<?, ?B/s]\n",
      "100%|██████████| 77.2M/77.2M [00:00<00:00, 854MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "! unzip -o -q imdb-dataset-of-50k-movie-reviews.zip -d Data\n",
    "! rm imdb-dataset-of-50k-movie-reviews.zip\n",
    "\n",
    "! kaggle datasets download -d stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\n",
    "! unzip -o -q rotten-tomatoes-movies-and-critic-reviews-dataset.zip -d Data\n",
    "! rm rotten-tomatoes-movies-and-critic-reviews-dataset.zip\n",
    "! rm Data/rotten_tomatoes_movies.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227e7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Embedding, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3574c63",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "- IMDB Review:<a href=\"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\"> Kaggle </a>\n",
    "\n",
    "- Rotten Tomatoes Review: <a href = \"https://www.kaggle.com/datasets/stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\">Kaggle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46eaebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset IMDB Dataset.csv berhasil dimuat dari folder Data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_folder = 'Data'\n",
    "datasets_file = 'IMDB Dataset.csv'\n",
    "datasets_path = f'{datasets_folder}/{datasets_file}'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(datasets_path)\n",
    "    print(f\"File dataset {datasets_file} berhasil dimuat dari folder {datasets_folder}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Tidak ada file {datasets_file} di dalam folder {datasets_folder}.\")\n",
    "\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d04d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset rotten_tomatoes_critic_reviews.csv berhasil dimuat dari folder Data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Andrew L. Urban</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Louise Keller</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>FILMINK (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ethan Alter</td>\n",
       "      <td>True</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rotten_tomatoes_link      critic_name  top_critic           publisher_name  \\\n",
       "0            m/0814255  Andrew L. Urban       False           Urban Cinefile   \n",
       "1            m/0814255    Louise Keller       False           Urban Cinefile   \n",
       "2            m/0814255              NaN       False      FILMINK (Australia)   \n",
       "3            m/0814255     Ben McEachen       False  Sunday Mail (Australia)   \n",
       "4            m/0814255      Ethan Alter        True       Hollywood Reporter   \n",
       "\n",
       "  review_type review_score review_date  \\\n",
       "0       Fresh          NaN  2010-02-06   \n",
       "1       Fresh          NaN  2010-02-06   \n",
       "2       Fresh          NaN  2010-02-09   \n",
       "3       Fresh        3.5/5  2010-02-09   \n",
       "4      Rotten          NaN  2010-02-10   \n",
       "\n",
       "                                      review_content  \n",
       "0  A fantasy adventure that fuses Greek mythology...  \n",
       "1  Uma Thurman as Medusa, the gorgon with a coiff...  \n",
       "2  With a top-notch cast and dazzling special eff...  \n",
       "3  Whether audiences will get behind The Lightnin...  \n",
       "4  What's really lacking in The Lightning Thief i...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_folder = 'Data'\n",
    "datasets_file = 'rotten_tomatoes_critic_reviews.csv'\n",
    "datasets_path = f'{datasets_folder}/{datasets_file}'\n",
    "\n",
    "try:\n",
    "    to_df = pd.read_csv(datasets_path)\n",
    "    print(f\"File dataset {datasets_file} berhasil dimuat dari folder {datasets_folder}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Tidak ada file {datasets_file} di dalam folder {datasets_folder}.\")\n",
    "\n",
    "to_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f1cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), (1130017, 8))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, to_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Andrew L. Urban</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Louise Keller</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>FILMINK (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ethan Alter</td>\n",
       "      <td>True</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rotten_tomatoes_link      critic_name  top_critic           publisher_name  \\\n",
       "0            m/0814255  Andrew L. Urban       False           Urban Cinefile   \n",
       "1            m/0814255    Louise Keller       False           Urban Cinefile   \n",
       "2            m/0814255              NaN       False      FILMINK (Australia)   \n",
       "3            m/0814255     Ben McEachen       False  Sunday Mail (Australia)   \n",
       "4            m/0814255      Ethan Alter        True       Hollywood Reporter   \n",
       "\n",
       "  review_type review_score review_date  \\\n",
       "0       Fresh          NaN  2010-02-06   \n",
       "1       Fresh          NaN  2010-02-06   \n",
       "2       Fresh          NaN  2010-02-09   \n",
       "3       Fresh        3.5/5  2010-02-09   \n",
       "4      Rotten          NaN  2010-02-10   \n",
       "\n",
       "                                      review_content  \n",
       "0  A fantasy adventure that fuses Greek mythology...  \n",
       "1  Uma Thurman as Medusa, the gorgon with a coiff...  \n",
       "2  With a top-notch cast and dazzling special eff...  \n",
       "3  Whether audiences will get behind The Lightnin...  \n",
       "4  What's really lacking in The Lightning Thief i...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_type\n",
       "Fresh     720210\n",
       "Rotten    409807\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df.review_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rotten</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_type                                     review_content\n",
       "0       Fresh  A fantasy adventure that fuses Greek mythology...\n",
       "1       Fresh  Uma Thurman as Medusa, the gorgon with a coiff...\n",
       "2       Fresh  With a top-notch cast and dazzling special eff...\n",
       "3       Fresh  Whether audiences will get behind The Lightnin...\n",
       "4      Rotten  What's really lacking in The Lightning Thief i..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = to_df[[\"review_type\", \"review_content\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Before everything we have to clean and process on our data before applying any machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert String values to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  label\n",
       "0  One of the other reviewers has mentioned that ...  positive      1\n",
       "1  A wonderful little production. <br /><br />The...  positive      1\n",
       "2  I thought this was a wonderful way to spend ti...  positive      1\n",
       "3  Basically there's a family where a little boy ...  negative      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15864\\4163341120.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1[\"label\"] = df1[\"review_type\"].apply(lambda x: 1 if x == \"Fresh\" else 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rotten</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_type                                     review_content  label\n",
       "0       Fresh  A fantasy adventure that fuses Greek mythology...      1\n",
       "1       Fresh  Uma Thurman as Medusa, the gorgon with a coiff...      1\n",
       "2       Fresh  With a top-notch cast and dazzling special eff...      1\n",
       "3       Fresh  Whether audiences will get behind The Lightnin...      1\n",
       "4      Rotten  What's really lacking in The Lightning Thief i...      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"label\"] = df1[\"review_type\"].apply(lambda x: 1 if x == \"Fresh\" else 0)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n"
     ]
    }
   ],
   "source": [
    "review = re.sub(r'^RT[\\s]+', '', df.iloc[1][\"review\"])\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great masters of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional dream techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwells murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = re.sub(r'<br />', '', review)\n",
    "review = review.replace(\"\\'\", \"\")\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'wonderful', 'little', 'production.', 'The', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-BBC', 'fashion', 'and', 'gives', 'a', 'comforting,', 'and', 'sometimes', 'discomforting,', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece.', 'The', 'actors', 'are', 'extremely', 'well', 'chosen-', 'Michael', 'Sheen', 'not', 'only', '\"has', 'got', 'all', 'the', 'polari\"', 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too!', 'You', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'Williams', 'diary', 'entries,', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece.', 'A', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'masters', 'of', 'comedy', 'and', 'his', 'life.', 'The', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things:', 'the', 'fantasy', 'of', 'the', 'guard', 'which,', 'rather', 'than', 'use', 'the', 'traditional', 'dream', 'techniques', 'remains', 'solid', 'then', 'disappears.', 'It', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses,', 'particularly', 'with', 'the', 'scenes', 'concerning', 'Orton', 'and', 'Halliwell', 'and', 'the', 'sets', '(particularly', 'of', 'their', 'flat', 'with', 'Halliwells', 'murals', 'decorating', 'every', 'surface)', 'are', 'terribly', 'well', 'done.']\n"
     ]
    }
   ],
   "source": [
    "tokens = review.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words(\"english\")\n",
    "\n",
    "cleaned_worlds = []\n",
    "\n",
    "for x in tokens:\n",
    "    if (x not in stopwords_english and string.punctuation):\n",
    "        cleaned_worlds.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 156)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_worlds), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "text_stem = []\n",
    "\n",
    "for x in cleaned_worlds:\n",
    "    stem_word = stemmer.stem(x)\n",
    "    text_stem.append(stem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ----- a\n",
      "wonderful ----- wonder\n",
      "little ----- littl\n",
      "production. ----- production.\n",
      "The ----- the\n",
      "filming ----- film\n",
      "technique ----- techniqu\n",
      "unassuming- ----- unassuming-\n",
      "old-time-BBC ----- old-time-bbc\n",
      "fashion ----- fashion\n",
      "gives ----- give\n",
      "comforting, ----- comforting,\n",
      "sometimes ----- sometim\n",
      "discomforting, ----- discomforting,\n",
      "sense ----- sens\n",
      "realism ----- realism\n",
      "entire ----- entir\n",
      "piece. ----- piece.\n",
      "The ----- the\n",
      "actors ----- actor\n",
      "extremely ----- extrem\n",
      "well ----- well\n",
      "chosen- ----- chosen-\n",
      "Michael ----- michael\n",
      "Sheen ----- sheen\n",
      "\"has ----- \"ha\n",
      "got ----- got\n",
      "polari\" ----- polari\"\n",
      "voices ----- voic\n",
      "pat ----- pat\n",
      "too! ----- too!\n",
      "You ----- you\n",
      "truly ----- truli\n",
      "see ----- see\n",
      "seamless ----- seamless\n",
      "editing ----- edit\n",
      "guided ----- guid\n",
      "references ----- refer\n",
      "Williams ----- william\n",
      "diary ----- diari\n",
      "entries, ----- entries,\n",
      "well ----- well\n",
      "worth ----- worth\n",
      "watching ----- watch\n",
      "terrificly ----- terrificli\n",
      "written ----- written\n",
      "performed ----- perform\n",
      "piece. ----- piece.\n",
      "A ----- a\n",
      "masterful ----- master\n",
      "production ----- product\n",
      "one ----- one\n",
      "great ----- great\n",
      "masters ----- master\n",
      "comedy ----- comedi\n",
      "life. ----- life.\n",
      "The ----- the\n",
      "realism ----- realism\n",
      "really ----- realli\n",
      "comes ----- come\n",
      "home ----- home\n",
      "little ----- littl\n",
      "things: ----- things:\n",
      "fantasy ----- fantasi\n",
      "guard ----- guard\n",
      "which, ----- which,\n",
      "rather ----- rather\n",
      "use ----- use\n",
      "traditional ----- tradit\n",
      "dream ----- dream\n",
      "techniques ----- techniqu\n",
      "remains ----- remain\n",
      "solid ----- solid\n",
      "disappears. ----- disappears.\n",
      "It ----- it\n",
      "plays ----- play\n",
      "knowledge ----- knowledg\n",
      "senses, ----- senses,\n",
      "particularly ----- particularli\n",
      "scenes ----- scene\n",
      "concerning ----- concern\n",
      "Orton ----- orton\n",
      "Halliwell ----- halliwel\n",
      "sets ----- set\n",
      "(particularly ----- (particularli\n",
      "flat ----- flat\n",
      "Halliwells ----- halliwel\n",
      "murals ----- mural\n",
      "decorating ----- decor\n",
      "every ----- everi\n",
      "surface) ----- surface)\n",
      "terribly ----- terribl\n",
      "well ----- well\n",
      "done. ----- done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cleaned_worlds)):\n",
    "    print(f\"{cleaned_worlds[i]} ----- {text_stem[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    text = re.sub(r'<br />', '', text)\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    \n",
    "    tokens = text.split()\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    cleaned_words = []\n",
    "\n",
    "    for x in tokens:\n",
    "        if (x not in stopwords_english and string.punctuation):\n",
    "            stem_word = stemmer.stem(x)\n",
    "            cleaned_words.append(stem_word)\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"review\"].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting first 120,000 data of rotten tomato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[:120000]\n",
    "df1[\"cleaned_text\"] = df1[\"review_content\"].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_content</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "      <td>1</td>\n",
       "      <td>a fantasi adventur fuse greek mytholog contemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "      <td>1</td>\n",
       "      <td>uma thurman medusa, gorgon coiffur writh snake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "      <td>1</td>\n",
       "      <td>with top-notch cast dazzl special effects, tid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fresh</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>1</td>\n",
       "      <td>whether audienc get behind the lightn thief ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rotten</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "      <td>0</td>\n",
       "      <td>what realli lack the lightn thief genuin sens ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_type                                     review_content  label  \\\n",
       "0       Fresh  A fantasy adventure that fuses Greek mythology...      1   \n",
       "1       Fresh  Uma Thurman as Medusa, the gorgon with a coiff...      1   \n",
       "2       Fresh  With a top-notch cast and dazzling special eff...      1   \n",
       "3       Fresh  Whether audiences will get behind The Lightnin...      1   \n",
       "4      Rotten  What's really lacking in The Lightning Thief i...      0   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  a fantasi adventur fuse greek mytholog contemp...  \n",
       "1  uma thurman medusa, gorgon coiffur writh snake...  \n",
       "2  with top-notch cast dazzl special effects, tid...  \n",
       "3  whether audienc get behind the lightn thief ha...  \n",
       "4  what realli lack the lightn thief genuin sens ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "train_x_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "test_x_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "max_length = 100\n",
    "train_x_padded = pad_sequences(train_x_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_x_padded = pad_sequences(test_x_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tomato = df1[\"cleaned_text\"]\n",
    "tokenizer.fit_on_texts(train_x_tomato)\n",
    "train_x_tomato_sequences = tokenizer.texts_to_sequences(train_x_tomato)\n",
    "train_x_tomato_padded = pad_sequences(train_x_tomato_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x_tomato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 64)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 64)           33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 64)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386,113\n",
      "Trainable params: 386,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 78s 60ms/step - loss: 0.4281 - accuracy: 0.7974 - val_loss: 0.3405 - val_accuracy: 0.8533\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 0.3129 - accuracy: 0.8716 - val_loss: 0.3273 - val_accuracy: 0.8543\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 0.2705 - accuracy: 0.8915 - val_loss: 0.3536 - val_accuracy: 0.8378\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 92s 73ms/step - loss: 0.2340 - accuracy: 0.9075 - val_loss: 0.3709 - val_accuracy: 0.8525\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 81s 65ms/step - loss: 0.2086 - accuracy: 0.9202 - val_loss: 0.3951 - val_accuracy: 0.8452\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 81s 65ms/step - loss: 0.1774 - accuracy: 0.9341 - val_loss: 0.4364 - val_accuracy: 0.8466\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 80s 64ms/step - loss: 0.1532 - accuracy: 0.9460 - val_loss: 0.4415 - val_accuracy: 0.8383\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 81s 64ms/step - loss: 0.1355 - accuracy: 0.9546 - val_loss: 0.5029 - val_accuracy: 0.8345\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 81s 64ms/step - loss: 0.1230 - accuracy: 0.9591 - val_loss: 0.5104 - val_accuracy: 0.8385\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 80s 64ms/step - loss: 0.1093 - accuracy: 0.9651 - val_loss: 0.6503 - val_accuracy: 0.8176\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x_padded, train_y, epochs=10, batch_size=32, validation_data=(test_x_padded, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 22ms/step - loss: 0.6503 - accuracy: 0.8176\n",
      "Accuracy: 0.8176000118255615\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_x_padded, test_y)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking some Sample Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 681ms/step\n",
      "[[0.9240088]\n",
      " [0.9369654]\n",
      " [0.9212563]]\n"
     ]
    }
   ],
   "source": [
    "text_sample = [\n",
    "    \"It was an awful movie but I liked it.\",\n",
    "    \"It was a good movie\",\n",
    "    \"I think it was the only movie that I could see until the end because it wasn't like the other movies that I have ever seen\"\n",
    "]\n",
    "\n",
    "text_sample_sequences = tokenizer.texts_to_sequences(text_sample)\n",
    "max_length = 100\n",
    "text_sample_padded = pad_sequences(text_sample_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "predictions = model.predict(text_sample_padded)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(max_length,))\n",
    "embedding = Embedding(vocab_size, latent_dim, input_length=max_length)(inputs)\n",
    "encoded = LSTM(latent_dim, return_sequences=False)(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = RepeatVector(max_length)(encoded)\n",
    "decoded = LSTM(latent_dim, return_sequences=True)(decoded)\n",
    "decoded = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 100, 64)           320000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 100, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100, 64)           33024     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 5000)        325000    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 711,048\n",
      "Trainable params: 711,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 540s 2s/step - loss: 1.7277 - val_loss: 0.7552\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 535s 2s/step - loss: 0.8472 - val_loss: 0.7343\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 532s 2s/step - loss: 0.8322 - val_loss: 0.7277\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 534s 2s/step - loss: 0.8275 - val_loss: 0.7216\n",
      "Epoch 5/10\n",
      " 17/313 [>.............................] - ETA: 9:00 - loss: 0.8127"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msequence_autoencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_tomato_padded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_tomato_padded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\engine\\training.py:1685\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1677\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1678\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1679\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1682\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1683\u001b[39m ):\n\u001b[32m   1684\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1687\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    891\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    897\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    923\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    924\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    925\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    928\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    929\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    930\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[39m, in \u001b[36mTracingCompiler.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m    141\u001b[39m   (concrete_function,\n\u001b[32m    142\u001b[39m    filtered_flat_args) = \u001b[38;5;28mself\u001b[39m._maybe_define_function(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcrete_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, args, captured_inputs, cancellation_manager)\u001b[39m\n\u001b[32m   1753\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1755\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1756\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1758\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1759\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1760\u001b[39m     args,\n\u001b[32m   1761\u001b[39m     possible_gradient_type,\n\u001b[32m   1762\u001b[39m     executing_eagerly)\n\u001b[32m   1763\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[39m, in \u001b[36m_EagerDefinedFunction.call\u001b[39m\u001b[34m(self, ctx, args, cancellation_manager)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    380\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    388\u001b[39m     outputs = execute.execute_with_cancellation(\n\u001b[32m    389\u001b[39m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.signature.name),\n\u001b[32m    390\u001b[39m         num_outputs=\u001b[38;5;28mself\u001b[39m._num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m         ctx=ctx,\n\u001b[32m    394\u001b[39m         cancellation_manager=cancellation_manager)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     51\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     55\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.fit(train_x_tomato_padded[:50000], np.expand_dims(train_x_tomato_padded[:50000], -1),\n",
    "                         epochs=10,\n",
    "                         batch_size=128,\n",
    "                         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      " Layer (type)                Output Shape              Param #   \n",
      "\n",
      "=================================================================\n",
      "\n",
      " embedding_16 (Embedding)    (None, 100, 64)           320000    \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_23 (LSTM)              (None, 100, 64)           33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_22 (Dropout)        (None, 100, 64)           0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_24 (LSTM)              (None, 64)                33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_23 (Dropout)        (None, 64)                0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "\n",
      "                                                                 \n",
      "\n",
      "=================================================================\n",
      "\n",
      "Total params: 386113 (1.47 MB)\n",
      "\n",
      "Trainable params: 386113 (1.47 MB)\n",
      "\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/10\n",
      "\n",
      "250/250 [==============================] - 85s 329ms/step - loss: 0.6949 - accuracy: 0.5014 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "250/250 [==============================] - 79s 318ms/step - loss: 0.6937 - accuracy: 0.5035 - val_loss: 0.6905 - val_accuracy: 0.5176\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "250/250 [==============================] - 79s 317ms/step - loss: 0.6531 - accuracy: 0.5953 - val_loss: 0.5103 - val_accuracy: 0.7744\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "250/250 [==============================] - 81s 325ms/step - loss: 0.4423 - accuracy: 0.8095 - val_loss: 0.4086 - val_accuracy: 0.8235\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "250/250 [==============================] - 79s 316ms/step - loss: 0.3497 - accuracy: 0.8549 - val_loss: 0.3715 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "250/250 [==============================] - 79s 317ms/step - loss: 0.3041 - accuracy: 0.8764 - val_loss: 0.3492 - val_accuracy: 0.8449\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "250/250 [==============================] - 79s 315ms/step - loss: 0.2727 - accuracy: 0.8918 - val_loss: 0.3622 - val_accuracy: 0.8471\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 0.2504 - accuracy: 0.9023 - val_loss: 0.3745 - val_accuracy: 0.8511\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "250/250 [==============================] - 87s 349ms/step - loss: 0.2327 - accuracy: 0.9102 - val_loss: 0.3656 - val_accuracy: 0.8430\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "250/250 [==============================] - 80s 321ms/step - loss: 0.2121 - accuracy: 0.9193 - val_loss: 0.3940 - val_accuracy: 0.8474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2afedca7dd0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=latent_dim, input_length=max_length),\n",
    "    LSTM(latent_dim, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(latent_dim),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.layers[1].set_weights(encoder.layers[2].get_weights())\n",
    "model.layers[3].set_weights(encoder.layers[2].get_weights())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_x_padded[:, :-1], train_y,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_x_padded, test_y)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "y_pred = (model.predict(test_x_padded) > 0.5).astype(\"int32\")\n",
    "print(f'Accuracy: {accuracy_score(test_y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "\n",
      "[[0.01109009]\n",
      "\n",
      " [0.86672544]\n",
      "\n",
      " [0.74528325]]\n",
      "\n",
      "Text: It was awful movie but I liked it.\n",
      "\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "\n",
      "\n",
      "Text: It was a good movie\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "\n",
      "Text: I think it was the only movie that I could see until the end because it wasn't like the other movies that I have ever seen\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_sample = [\n",
    "    \"It was awful movie but I liked it.\",\n",
    "    \"It was a good movie\",\n",
    "    \"I think it was the only movie that I could see until the end because it wasn't like the other movies that I have ever seen\"\n",
    "]\n",
    "text_sample_cleaned = [preprocessing(text) for text in text_sample]\n",
    "\n",
    "text_sample_sequences = tokenizer.texts_to_sequences(text_sample_cleaned)\n",
    "text_sample_padded = pad_sequences(text_sample_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "predictions = model.predict(text_sample_padded)\n",
    "print(predictions)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Text: {text_sample[i]}')\n",
    "    print(f'Predicted Sentiment: {\"Positive\" if prediction > 0.5 else \"Negative\"}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirmahdiAbtl\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.save('sequence_autoencoder.h5')\n",
    "encoder.save('encoder.h5')\n",
    "model.save('sentiment_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_sequence_autoencoder = load_model('sequence_autoencoder.h5')\n",
    "loaded_encoder = load_model('encoder.h5')\n",
    "loaded_sentiment_classifier = load_model('sentiment_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "pretrained_model = hub.KerasLayer(pretrained_model_url, trainable=False)\n",
    "\n",
    "train_embeddings = np.array([pretrained_model([text]).numpy()[0] for text in train_x])\n",
    "test_embeddings = np.array([pretrained_model([text]).numpy()[0] for text in test_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      " Layer (type)                Output Shape              Param #   \n",
      "\n",
      "=================================================================\n",
      "\n",
      " reshape (Reshape)           (None, 1, 512)            0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_6 (LSTM)               (None, 1, 64)             147712    \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_2 (Dropout)         (None, 1, 64)             0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_7 (LSTM)               (None, 64)                33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "\n",
      "                                                                 \n",
      "\n",
      "=================================================================\n",
      "\n",
      "Total params: 180801 (706.25 KB)\n",
      "\n",
      "Trainable params: 180801 (706.25 KB)\n",
      "\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LM_LSTM_model = Sequential([\n",
    "    InputLayer(input_shape=(train_embeddings.shape[1],)),\n",
    "    tf.keras.layers.Reshape((1, train_embeddings.shape[1])),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "LM_LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "LM_LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "250/250 [==============================] - 4s 7ms/step - loss: 0.4946 - accuracy: 0.7674 - val_loss: 0.4074 - val_accuracy: 0.8129\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4097 - accuracy: 0.8119 - val_loss: 0.4046 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4056 - accuracy: 0.8150 - val_loss: 0.4011 - val_accuracy: 0.8144\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4018 - accuracy: 0.8168 - val_loss: 0.3993 - val_accuracy: 0.8170\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3999 - accuracy: 0.8163 - val_loss: 0.3995 - val_accuracy: 0.8179\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3978 - accuracy: 0.8172 - val_loss: 0.4013 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3952 - accuracy: 0.8205 - val_loss: 0.4019 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3935 - accuracy: 0.8183 - val_loss: 0.3964 - val_accuracy: 0.8174\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3920 - accuracy: 0.8209 - val_loss: 0.3977 - val_accuracy: 0.8188\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.3914 - accuracy: 0.8209 - val_loss: 0.3981 - val_accuracy: 0.8194\n",
      "\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4037 - accuracy: 0.8124\n",
      "\n",
      "Test Loss: 0.40373358130455017\n",
      "\n",
      "Test Accuracy: 0.8123999834060669\n",
      "\n",
      "313/313 [==============================] - 1s 1ms/step\n",
      "\n",
      "Accuracy: 0.8124\n"
     ]
    }
   ],
   "source": [
    "LM_LSTM_model.fit(train_embeddings, train_y, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = LM_LSTM_model.evaluate(test_embeddings, test_y)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = (LM_LSTM_model.predict(test_embeddings) > 0.5).astype(\"int32\")\n",
    "print(f'Accuracy: {accuracy_score(test_y, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "\n",
      "[[0.54970556]\n",
      "\n",
      " [0.7660237 ]\n",
      "\n",
      " [0.8268357 ]]\n",
      "\n",
      "Text: It was awful movie but i liked it.\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "\n",
      "Text: it was a good movie\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n",
      "\n",
      "Text: i think it was the only movie that I could see until the end because it wan't like the other movies that I have ever seen\n",
      "\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_sample = [\n",
    "    \"It was awful movie but i liked it.\",\n",
    "    \"it was a good movie\",\n",
    "    \"i think it was the only movie that I could see until the end because it wan't like the other movies that I have ever seen\"\n",
    "]\n",
    "text_sample_cleaned = [preprocessing(text) for text in text_sample]\n",
    "\n",
    "text_sample_embeddings = np.array([pretrained_model([text]).numpy()[0] for text in text_sample_cleaned])\n",
    "\n",
    "predictions = LM_LSTM_model.predict(text_sample_embeddings)\n",
    "print(predictions)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f'Text: {text_sample[i]}')\n",
    "    print(f'Predicted Sentiment: {\"Positive\" if prediction > 0.5 else \"Negative\"}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Language model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      " Layer (type)                Output Shape              Param #   \n",
      "\n",
      "=================================================================\n",
      "\n",
      " input_11 (InputLayer)       [(None, 100)]             0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " embedding_8 (Embedding)     (None, 100, 64)           320000    \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_8 (LSTM)               (None, 64)                33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dense_6 (Dense)             (None, 5000)              325000    \n",
      "\n",
      "                                                                 \n",
      "\n",
      "=================================================================\n",
      "\n",
      "Total params: 678024 (2.59 MB)\n",
      "\n",
      "Trainable params: 678024 (2.59 MB)\n",
      "\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(max_length,)) \n",
    "embedding = Embedding(vocab_size, latent_dim, input_length=max_length)(inputs)\n",
    "lstm = LSTM(latent_dim)(embedding)\n",
    "output = Dense(vocab_size, activation='softmax')(lstm)\n",
    "\n",
    "language_model = Model(inputs, output)\n",
    "language_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "language_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "250/250 [==============================] - 58s 225ms/step - loss: 6.6027 - accuracy: 0.1382 - val_loss: 6.1599 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 6.1601 - accuracy: 0.1388 - val_loss: 6.1849 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 6.1372 - accuracy: 0.1400 - val_loss: 6.2115 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "250/250 [==============================] - 49s 194ms/step - loss: 6.1173 - accuracy: 0.1388 - val_loss: 6.2402 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "250/250 [==============================] - 48s 192ms/step - loss: 6.1060 - accuracy: 0.1400 - val_loss: 6.2617 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "250/250 [==============================] - 49s 196ms/step - loss: 6.0972 - accuracy: 0.1388 - val_loss: 6.2817 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 6.0882 - accuracy: 0.1401 - val_loss: 6.2998 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "250/250 [==============================] - 48s 191ms/step - loss: 6.0791 - accuracy: 0.1400 - val_loss: 6.3178 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 6.0649 - accuracy: 0.1399 - val_loss: 6.3367 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 6.0452 - accuracy: 0.1404 - val_loss: 6.3611 - val_accuracy: 0.1554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2afe99b44d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_padded = pad_sequences(train_x_sequences, maxlen=max_length + 1, padding='post', truncating='post') \n",
    "test_x_padded = pad_sequences(test_x_sequences, maxlen=max_length + 1, padding='post', truncating='post')\n",
    "\n",
    "language_model = Model(inputs, output)\n",
    "language_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "language_model.fit(\n",
    "    train_x_padded[:, :-1], \n",
    "    train_x_padded[:, 1:].reshape(-1), \n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      " Layer (type)                Output Shape              Param #   \n",
      "\n",
      "=================================================================\n",
      "\n",
      " embedding_13 (Embedding)    (None, 100, 64)           320000    \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_17 (LSTM)              (None, 100, 64)           33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_18 (Dropout)        (None, 100, 64)           0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " lstm_18 (LSTM)              (None, 64)                33024     \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "\n",
      "                                                                 \n",
      "\n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "\n",
      "                                                                 \n",
      "\n",
      "=================================================================\n",
      "\n",
      "Total params: 386113 (1.47 MB)\n",
      "\n",
      "Trainable params: 386113 (1.47 MB)\n",
      "\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/10\n",
      "\n",
      "250/250 [==============================] - 75s 286ms/step - loss: 0.6918 - accuracy: 0.5174 - val_loss: 0.6637 - val_accuracy: 0.5946\n",
      "\n",
      "Epoch 2/10\n",
      "\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 0.6789 - accuracy: 0.5501 - val_loss: 0.6915 - val_accuracy: 0.5344\n",
      "\n",
      "Epoch 3/10\n",
      "\n",
      "250/250 [==============================] - 65s 258ms/step - loss: 0.6815 - accuracy: 0.5546 - val_loss: 0.6483 - val_accuracy: 0.5947\n",
      "\n",
      "Epoch 4/10\n",
      "\n",
      "250/250 [==============================] - 65s 259ms/step - loss: 0.5128 - accuracy: 0.7523 - val_loss: 0.4636 - val_accuracy: 0.7993\n",
      "\n",
      "Epoch 5/10\n",
      "\n",
      "250/250 [==============================] - 80s 322ms/step - loss: 0.4128 - accuracy: 0.8249 - val_loss: 0.4304 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 6/10\n",
      "\n",
      "250/250 [==============================] - 91s 364ms/step - loss: 0.3683 - accuracy: 0.8493 - val_loss: 0.4435 - val_accuracy: 0.8060\n",
      "\n",
      "Epoch 7/10\n",
      "\n",
      "250/250 [==============================] - 92s 365ms/step - loss: 0.3377 - accuracy: 0.8640 - val_loss: 0.4047 - val_accuracy: 0.8236\n",
      "\n",
      "Epoch 8/10\n",
      "\n",
      "250/250 [==============================] - 100s 401ms/step - loss: 0.3166 - accuracy: 0.8757 - val_loss: 0.3844 - val_accuracy: 0.8325\n",
      "\n",
      "Epoch 9/10\n",
      "\n",
      "250/250 [==============================] - 77s 310ms/step - loss: 0.2966 - accuracy: 0.8828 - val_loss: 0.3983 - val_accuracy: 0.8372\n",
      "\n",
      "Epoch 10/10\n",
      "\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.2824 - accuracy: 0.8890 - val_loss: 0.3935 - val_accuracy: 0.8355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2aff0a574d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LM = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=latent_dim, input_length=max_length),\n",
    "    LSTM(latent_dim, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(latent_dim),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_LM.layers[1].set_weights(language_model.layers[2].get_weights())\n",
    "model_LM.layers[3].set_weights(language_model.layers[2].get_weights())\n",
    "\n",
    "model_LM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_LM.summary()\n",
    "\n",
    "model_LM.fit(train_x_padded[:, :-1], train_y,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
